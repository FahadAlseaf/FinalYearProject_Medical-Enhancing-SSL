# Improving Medical Imaging With SSL Image Translation

![Python](https://img.shields.io/badge/Python-3.10-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c)
![Status](https://img.shields.io/badge/Status-Completed-success)
![License](https://img.shields.io/badge/License-MIT-green)

This repository contains the official implementation for the Final Year Project: **Improving Medical Imaging With SSL Image Translation**.

The project introduces the **MIRAM (Masked Image Reconstruction Across Multiple Scales)** framework, utilizing Self-Supervised Learning (SSL) to enhance medical imaging quality without relying on massive labeled datasets.

## ğŸ“„ Abstract

This study investigates a self-supervised image-to-image translation approach for medical imaging. The method utilizes a Generative Adversarial Network (GAN) trained with reconstruction, adversarial, and cycle-consistency losses. We introduce the **MIRAM** architecture combined with Vision Transformers (ViT) and Masked Autoencoders (MAE) to learn robust anatomical features from unlabeled data.

Experimental results on X-ray, CT, and MRI datasets show significant gains, achieving a **4.2 dB increase in PSNR** and a **12% improvement in SSIM** compared to baseline methods.

## ğŸ‘¥ Authors (Qassim University)

- **Ahmed Alhabib**
- **Fahad Alseaf**
- **Meshal Albaradi**

**Supervisor:** Dr. Mohmmad Ali A. Hammoudeh

---

## ğŸ“Š Results

| Method | PSNR (dB) | SSIM |
|--------|-----------|------|
| Bicubic Interpolation | 28.4 | 0.82 |
| Baseline (No SSL) | 31.2 | 0.88 |
| **Ours (MIRAM + SSL)** | **35.4** | **0.94** |

---

## ğŸ“‚ Repository Structure

```
medical-ssl-sr/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ module_1_general_ssl/        # X-Ray/CT Enhancement
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ config.py                # Centralized configuration
â”‚   â”œâ”€â”€ pretrain.py              # Phase 1: MAE pre-training
â”‚   â”œâ”€â”€ train.py                 # Phase 2: Fine-tuning with SSL
â”‚   â”œâ”€â”€ train_baseline.py        # Phase 3: Baseline comparison
â”‚   â””â”€â”€ visualize_results.py     # Inference & visualization
â”‚
â””â”€â”€ module_2_mri_super_res/      # Brain MRI Super-Resolution
    â”œâ”€â”€ README.md
    â”œâ”€â”€ config.py                # Centralized configuration
    â”œâ”€â”€ models.py                # MIRAM Generator & Discriminator
    â”œâ”€â”€ losses.py                # Custom loss functions
    â”œâ”€â”€ dataset.py               # Data loading utilities
    â”œâ”€â”€ train.py                 # Training loop
    â”œâ”€â”€ evaluate.py              # Evaluation & metrics
    â””â”€â”€ enhance.py               # Single image inference
```

### Module Descriptions

#### 1. `module_1_general_ssl/`
**Focus:** Self-Supervised Pre-training & General Translation

- Implements **Masked Autoencoders (MAE)** with Vision Transformers (ViT)
- Used for the initial "Pre-training Phase" to learn anatomical features from unlabeled X-Ray/CT data
- Includes a baseline comparison script to demonstrate SSL effectiveness

#### 2. `module_2_mri_super_res/`
**Focus:** High-Precision Brain MRI Super-Resolution (4x)

- Implements the **MIRAM Generator** with Channel & Spatial Attention
- Optimized for **16-bit Medical TIFF** images
- Uses advanced loss functions: **Edge Loss + Charbonnier Loss + Perceptual Loss**

---

## ğŸš€ Quick Start

### 1. Clone & Install

```bash
git clone https://github.com/yourusername/medical-ssl-sr.git
cd medical-ssl-sr
pip install -r requirements.txt
```

### 2. Prepare Your Data

```bash
# Create data directory
mkdir -p data/medical_images

# Option A: Place your own X-Ray/CT images in data/medical_images/
# Option B: Download a public dataset (e.g., NIH ChestX-ray)
```

### 3. Configure Paths

Edit the configuration file for your chosen module:

```python
# module_1_general_ssl/config.py
DATA_DIR = "./data/medical_images"  # Update to your data path

# module_2_mri_super_res/config.py
DRIVE_PATH = "./data/mri_project"   # Update to your data path
```

Alternatively, use environment variables:
```bash
export DATA_DIR="/path/to/your/data"
export OUTPUT_DIR="/path/to/outputs"
```

### 4. Run Module 1 (X-Ray/CT Enhancement)

```bash
cd module_1_general_ssl

# Phase 1: Pre-train the encoder using MAE
python pretrain.py

# Phase 2: Fine-tune for super-resolution with SSL weights
python train.py

# Phase 3: Train baseline for comparison (no pre-training)
python train_baseline.py

# Visualize results
python visualize_results.py
```

### 5. Run Module 2 (MRI Super-Resolution)

```bash
cd module_2_mri_super_res

# Train the MIRAM model
python train.py

# Evaluate on test set
python evaluate.py

# Enhance a single image
python enhance.py
```

---

## âš™ï¸ Configuration

### Module 1 Settings (`module_1_general_ssl/config.py`)

| Parameter | Default | Description |
|-----------|---------|-------------|
| `DATA_DIR` | `./data` | Path to training images |
| `EPOCHS` | 50 | Training epochs |
| `BATCH_SIZE` | 16 | Batch size (adjust for VRAM) |
| `LEARNING_RATE` | 1e-4 | Learning rate |
| `IMG_SIZE` | 224 | Input image size |
| `MASK_RATIO` | 0.75 | MAE masking ratio |

### Module 2 Settings (`module_2_mri_super_res/config.py`)

| Parameter | Default | Description |
|-----------|---------|-------------|
| `SCALE_FACTOR` | 4 | Super-resolution scale |
| `CROP_SIZE` | 128 | Training patch size |
| `N_EPOCHS` | 200 | Training epochs |
| `USE_16BIT` | True | 16-bit TIFF support |
| `LAMBDA_EDGE` | 0.1 | Edge loss weight |

---

## ğŸ“‹ Requirements

- Python 3.10+
- PyTorch 2.0+
- CUDA-capable GPU (recommended: 8GB+ VRAM)
- See `requirements.txt` for full dependencies

---

## ğŸ“œ BibTeX

If you use this code in your research, please cite:

```bibtex
@thesis{Quni2025medical,
  title={Improving Medical Imaging With SSL Image Translation},
  author={Alhabib, Ahmed and Alseaf, Fahad and Albaradi, Meshal},
  year={2025},
  school={Qassim University}
}
```
---

## ğŸ™ Acknowledgments

- Dr. Mohmmad Ali A. Hammoudeh for supervision and guidance
- Qassim University for supporting this research
- The authors of MAE and Vision Transformer for foundational work
