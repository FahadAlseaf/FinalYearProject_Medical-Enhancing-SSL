# Module 3: MIRAM ViT-MAE (Masked Image Reconstruction Across Multiple Scales)

This module implements a **Vision Transformer-based Masked Autoencoder** for self-supervised learning on medical images, with applications in **image restoration** and **tumor classification**.

## ğŸ¯ Key Features

| Feature | Description |
|---------|-------------|
| **Self-Supervised Pre-training** | Learn anatomical features without labels using masked autoencoding |
| **Dual-Scale Reconstruction** | Reconstruct at both fine (224Ã—224) and coarse (112Ã—112) scales |
| **Attention Visualization** | Generate heatmaps showing where the model focuses |
| **Tumor Classification** | Fine-tune for binary classification with attention localization |
| **ONNX Export** | Deploy optimized model with speed benchmarking |

## ğŸ“‚ File Structure

```
module_3_miram_vit/
â”œâ”€â”€ config.py           # Centralized configuration
â”œâ”€â”€ models.py           # MIRAM architecture (ViT encoder + dual decoder)
â”œâ”€â”€ dataset.py          # Data loading utilities
â”œâ”€â”€ losses.py           # Dual-scale patch loss + metrics
â”œâ”€â”€ train.py            # Self-supervised pre-training
â”œâ”€â”€ evaluate.py         # Reconstruction visualization
â”œâ”€â”€ enhance.py          # Single image inference
â”œâ”€â”€ classify.py         # Tumor classification with heatmaps
â”œâ”€â”€ export_deploy.py    # ONNX export & benchmarking
â””â”€â”€ README.md           # This file
```

## ğŸ—ï¸ Architecture

### MIRAM Model

```
Input Image (1, 224, 224)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ENCODER                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Patch Embedding: 16Ã—16 patches â†’ 196 tokens            â”‚  â”‚
â”‚  â”‚  + Positional Embedding + [CLS] Token                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                     â”‚
â”‚                          â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Random Masking (75% hidden)                            â”‚  â”‚
â”‚  â”‚  â†’ Keep only 49 visible patches                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                     â”‚
â”‚                          â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  12Ã— Transformer Blocks                                 â”‚  â”‚
â”‚  â”‚  (384 dim, 6 heads, MLP ratio 4.0)                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DECODER                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Project to 256 dim + Add mask tokens                   â”‚  â”‚
â”‚  â”‚  Unshuffle to restore original order                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                     â”‚
â”‚                          â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4Ã— Decoder Transformer Blocks                          â”‚  â”‚
â”‚  â”‚  (256 dim, 8 heads)                                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                     â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â–¼                       â–¼                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Fine Head      â”‚     â”‚  Coarse Head    â”‚                  â”‚
â”‚  â”‚  (16Ã—16 patches)â”‚     â”‚  (8Ã—8 patches)  â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                           â”‚
        â–¼                           â–¼
   Fine Recon (224Ã—224)      Coarse Recon (112Ã—112)
```

## ğŸš€ Quick Start

### 1. Configure Paths

```python
# Option A: Edit config.py directly
DRIVE_PATH = "/path/to/your/data"

# Option B: Use environment variables
export MIRAM_DATA_PATH="/path/to/your/data"
export MIRAM_OUTPUT_PATH="/path/to/outputs"
```

### 2. Prepare Dataset

```
data/mri_project/Brain_MRI/
â”œâ”€â”€ HR/                    # High-resolution images
â”‚   â”œâ”€â”€ image001.tif
â”‚   â”œâ”€â”€ image002.tif
â”‚   â””â”€â”€ ...
â””â”€â”€ MASK/                  # Tumor masks (for classification)
    â”œâ”€â”€ image001.tif       # Non-zero pixels = tumor
    â”œâ”€â”€ image002.tif
    â””â”€â”€ ...
```

### 3. Train the Model

```bash
# Phase 1: Self-supervised pre-training
python train.py

# Phase 2: Evaluate reconstruction quality
python evaluate.py

# Phase 3: Train tumor classifier (optional)
python classify.py
```

### 4. Inference

```bash
# Enhance a single image
python enhance.py --input scan.tif --output restored.tif

# Or interactive mode
python enhance.py
```

### 5. Deploy

```bash
# Export to ONNX with benchmarking
python export_deploy.py
```

## âš™ï¸ Configuration

### Key Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `IMG_SIZE` | 224 | Input image size (ViT standard) |
| `PATCH_SIZE` | 16 | Patch size (224/16 = 14Ã—14 grid) |
| `EMBED_DIM` | 384 | Encoder embedding dimension |
| `DEPTH` | 12 | Number of encoder transformer blocks |
| `MASK_RATIO` | 0.75 | Fraction of patches to mask |
| `N_EPOCHS` | 200 | Pre-training epochs |

### Training Settings

| Parameter | Default | Description |
|-----------|---------|-------------|
| `BATCH_SIZE` | 16 | Training batch size |
| `LEARNING_RATE` | 1.5e-4 | AdamW learning rate |
| `WEIGHT_DECAY` | 0.05 | L2 regularization |
| `LAMBDA_FINE` | 1.0 | Fine-scale loss weight |
| `LAMBDA_COARSE` | 0.5 | Coarse-scale loss weight |

## ğŸ“Š Output Files

### Training Outputs

| File | Description |
|------|-------------|
| `best_miram_mae.pth` | Best pre-trained model weights |
| `loss_curve.png` | Training/validation loss plot |
| `miram_eval_sample.png` | Reconstruction visualization |

### Classification Outputs

| File | Description |
|------|-------------|
| `best_tumor_classifier.pth` | Trained classifier weights |
| `tumor_viz_*.png` | Attention heatmap visualizations |

### Deployment Outputs

| File | Description |
|------|-------------|
| `miram_model_optimized.onnx` | Exported ONNX model |

## ğŸ”¬ How It Works

### 1. Self-Supervised Pre-training

The model learns to reconstruct randomly masked patches:

1. **Masking**: 75% of patches are randomly hidden
2. **Encoding**: Visible patches processed by ViT encoder
3. **Decoding**: Predict pixel values for ALL patches
4. **Loss**: MSE only on masked patches (reconstruction target)

This forces the model to learn meaningful anatomical representations.

### 2. Tumor Classification

The pre-trained encoder is fine-tuned for classification:

1. **Feature Extraction**: Use [CLS] token from encoder
2. **Classification Head**: Linear layers â†’ sigmoid
3. **Attention Maps**: Visualize where model focuses

### 3. Attention Visualization

The attention weights from the last transformer block show which image regions influence the classification decision, providing interpretability for medical diagnosis.

## ğŸ“ˆ Expected Results

| Metric | Pre-training | Classification |
|--------|--------------|----------------|
| PSNR | ~30-35 dB | - |
| SSIM | ~0.90-0.95 | - |
| Accuracy | - | ~90-95% |
| Inference | - | <100ms (ONNX) |

## ğŸ”§ Troubleshooting

**"CUDA out of memory"**
- Reduce `BATCH_SIZE` in config.py
- Use gradient accumulation

**"Dataset empty"**
- Check `DATASET_PATH` points to correct location
- Ensure HR/ folder contains images

**"Model not found"**
- Run `train.py` first to generate weights

**"ONNX export failed"**
- Install: `pip install onnx onnxruntime`
- Check PyTorch version compatibility

## ğŸ“š References

- He et al., "Masked Autoencoders Are Scalable Vision Learners", CVPR 2022
- Dosovitskiy et al., "An Image is Worth 16x16 Words", ICLR 2021
